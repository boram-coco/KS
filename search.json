[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaggle Study",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 19, 2023\n\n\n[Kaggle] Santander Product Recommendation Competition\n\n\n김보람 \n\n\n\n\nDec 19, 2023\n\n\n[Kaggle] 02. Santander Product Recommendation Competition\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Santander_ver1.html",
    "href": "posts/Santander_ver1.html",
    "title": "[Kaggle] Santander Product Recommendation Competition_v1",
    "section": "",
    "text": "[Kaggle] Santander Product Recommendation Competition\nkaggle 우승작으로 배우는 머신러닝 탐구생활"
  },
  {
    "objectID": "posts/Santander_ver1.html#수치형-변수",
    "href": "posts/Santander_ver1.html#수치형-변수",
    "title": "[Kaggle] Santander Product Recommendation Competition_v1",
    "section": "수치형 변수",
    "text": "수치형 변수\n\nnum_cols = [col for col in df_train.columns[:24] if df_train[col].dtype in ['int64','float64']]\ndf_train[num_cols].describe()\n\n\n\n\n\n\n\n\nncodpers\nind_nuevo\nindrel\ntipodom\ncod_prov\nind_actividad_cliente\nrenta\n\n\n\n\ncount\n1.364731e+07\n1.361958e+07\n1.361958e+07\n13619574.0\n1.355372e+07\n1.361958e+07\n1.085293e+07\n\n\nmean\n8.349042e+05\n5.956184e-02\n1.178399e+00\n1.0\n2.657147e+01\n4.578105e-01\n1.342543e+05\n\n\nstd\n4.315650e+05\n2.366733e-01\n4.177469e+00\n0.0\n1.278402e+01\n4.982169e-01\n2.306202e+05\n\n\nmin\n1.588900e+04\n0.000000e+00\n1.000000e+00\n1.0\n1.000000e+00\n0.000000e+00\n1.202730e+03\n\n\n25%\n4.528130e+05\n0.000000e+00\n1.000000e+00\n1.0\n1.500000e+01\n0.000000e+00\n6.871098e+04\n\n\n50%\n9.318930e+05\n0.000000e+00\n1.000000e+00\n1.0\n2.800000e+01\n0.000000e+00\n1.018500e+05\n\n\n75%\n1.199286e+06\n0.000000e+00\n1.000000e+00\n1.0\n3.500000e+01\n1.000000e+00\n1.559560e+05\n\n\nmax\n1.553689e+06\n1.000000e+00\n9.900000e+01\n1.0\n5.200000e+01\n1.000000e+00\n2.889440e+07"
  },
  {
    "objectID": "posts/Santander_ver1.html#범주형-변수",
    "href": "posts/Santander_ver1.html#범주형-변수",
    "title": "[Kaggle] Santander Product Recommendation Competition_v1",
    "section": "범주형 변수",
    "text": "범주형 변수\n\ncat_cols = [col for col in df_train.columns[:24] if df_train[col].dtype in ['object']]\ndf_train[cat_cols].describe()\n\n\n\n\n\n\n\n\nfecha_dato\nind_empleado\npais_residencia\nsexo\nage\nfecha_alta\nantiguedad\nult_fec_cli_1t\nindrel_1mes\ntiprel_1mes\nindresi\nindext\nconyuemp\ncanal_entrada\nindfall\nnomprov\nsegmento\n\n\n\n\ncount\n13647309\n13619575\n13619575\n13619505\n13647309\n13619575\n13647309\n24793\n13497528.0\n13497528\n13619575\n13619575\n1808\n13461183\n13619575\n13553718\n13457941\n\n\nunique\n17\n5\n118\n2\n235\n6756\n507\n223\n13.0\n5\n2\n2\n2\n162\n2\n52\n3\n\n\ntop\n2016-05-28\nN\nES\nV\n23\n2014-07-28\n0\n2015-12-24\n1.0\nI\nS\nN\nN\nKHE\nN\nMADRID\n02 - PARTICULARES\n\n\nfreq\n931453\n13610977\n13553710\n7424252\n542682\n57389\n134335\n763\n7277607.0\n7304875\n13553711\n12974839\n1791\n4055270\n13584813\n4409600\n7960220\n\n\n\n\n\n\n\n- 고유값\n\nfor col in cat_cols:\n    uniq = np.unique(df_train[col].astype(str))\n    print('-' * 50)\n    print('# col {}, n_uniq {}, uniq {}'.format(col, len(uniq), uniq))\n\n--------------------------------------------------\n# col fecha_dato, n_uniq 17, uniq ['2015-01-28' '2015-02-28' '2015-03-28' '2015-04-28' '2015-05-28'\n '2015-06-28' '2015-07-28' '2015-08-28' '2015-09-28' '2015-10-28'\n '2015-11-28' '2015-12-28' '2016-01-28' '2016-02-28' '2016-03-28'\n '2016-04-28' '2016-05-28']\n--------------------------------------------------\n# col ind_empleado, n_uniq 6, uniq ['A' 'B' 'F' 'N' 'S' 'nan']\n--------------------------------------------------\n# col pais_residencia, n_uniq 119, uniq ['AD' 'AE' 'AL' 'AO' 'AR' 'AT' 'AU' 'BA' 'BE' 'BG' 'BM' 'BO' 'BR' 'BY'\n 'BZ' 'CA' 'CD' 'CF' 'CG' 'CH' 'CI' 'CL' 'CM' 'CN' 'CO' 'CR' 'CU' 'CZ'\n 'DE' 'DJ' 'DK' 'DO' 'DZ' 'EC' 'EE' 'EG' 'ES' 'ET' 'FI' 'FR' 'GA' 'GB'\n 'GE' 'GH' 'GI' 'GM' 'GN' 'GQ' 'GR' 'GT' 'GW' 'HK' 'HN' 'HR' 'HU' 'IE'\n 'IL' 'IN' 'IS' 'IT' 'JM' 'JP' 'KE' 'KH' 'KR' 'KW' 'KZ' 'LB' 'LT' 'LU'\n 'LV' 'LY' 'MA' 'MD' 'MK' 'ML' 'MM' 'MR' 'MT' 'MX' 'MZ' 'NG' 'NI' 'NL'\n 'NO' 'NZ' 'OM' 'PA' 'PE' 'PH' 'PK' 'PL' 'PR' 'PT' 'PY' 'QA' 'RO' 'RS'\n 'RU' 'SA' 'SE' 'SG' 'SK' 'SL' 'SN' 'SV' 'TG' 'TH' 'TN' 'TR' 'TW' 'UA'\n 'US' 'UY' 'VE' 'VN' 'ZA' 'ZW' 'nan']\n--------------------------------------------------\n# col sexo, n_uniq 3, uniq ['H' 'V' 'nan']\n--------------------------------------------------\n# col age, n_uniq 219, uniq ['  2' '  3' '  4' '  5' '  6' '  7' '  8' '  9' ' 10' ' 11' ' 12' ' 13'\n ' 14' ' 15' ' 16' ' 17' ' 18' ' 19' ' 20' ' 21' ' 22' ' 23' ' 24' ' 25'\n ' 26' ' 27' ' 28' ' 29' ' 30' ' 31' ' 32' ' 33' ' 34' ' 35' ' 36' ' 37'\n ' 38' ' 39' ' 40' ' 41' ' 42' ' 43' ' 44' ' 45' ' 46' ' 47' ' 48' ' 49'\n ' 50' ' 51' ' 52' ' 53' ' 54' ' 55' ' 56' ' 57' ' 58' ' 59' ' 60' ' 61'\n ' 62' ' 63' ' 64' ' 65' ' 66' ' 67' ' 68' ' 69' ' 70' ' 71' ' 72' ' 73'\n ' 74' ' 75' ' 76' ' 77' ' 78' ' 79' ' 80' ' 81' ' 82' ' 83' ' 84' ' 85'\n ' 86' ' 87' ' 88' ' 89' ' 90' ' 91' ' 92' ' 93' ' 94' ' 95' ' 96' ' 97'\n ' 98' ' 99' ' NA' '10' '100' '101' '102' '103' '104' '105' '106' '107'\n '108' '109' '11' '110' '111' '112' '113' '114' '115' '116' '117' '12'\n '126' '127' '13' '14' '15' '16' '163' '164' '17' '18' '19' '2' '20' '21'\n '22' '23' '24' '25' '26' '27' '28' '29' '3' '30' '31' '32' '33' '34' '35'\n '36' '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46' '47' '48' '49'\n '5' '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '6' '60' '61' '62'\n '63' '64' '65' '66' '67' '68' '69' '7' '70' '71' '72' '73' '74' '75' '76'\n '77' '78' '79' '8' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '9'\n '90' '91' '92' '93' '94' '95' '96' '97' '98' '99']\n--------------------------------------------------\n# col fecha_alta, n_uniq 6757, uniq ['1995-01-16' '1995-01-17' '1995-01-23' ... '2016-05-30' '2016-05-31'\n 'nan']\n--------------------------------------------------\n# col antiguedad, n_uniq 506, uniq ['      0' '      1' '      2' '      3' '      4' '      5' '      6'\n '      7' '      8' '      9' '     10' '     11' '     12' '     13'\n '     14' '     15' '     16' '     17' '     18' '     19' '     20'\n '     21' '     22' '     23' '     24' '     25' '     26' '     27'\n '     28' '     29' '     30' '     31' '     32' '     33' '     34'\n '     35' '     36' '     37' '     38' '     39' '     40' '     41'\n '     42' '     43' '     44' '     45' '     46' '     47' '     48'\n '     49' '     50' '     51' '     52' '     53' '     54' '     55'\n '     56' '     57' '     58' '     59' '     60' '     61' '     62'\n '     63' '     64' '     65' '     66' '     67' '     68' '     69'\n '     70' '     71' '     72' '     73' '     74' '     75' '     76'\n '     77' '     78' '     79' '     80' '     81' '     82' '     83'\n '     84' '     85' '     86' '     87' '     88' '     89' '     90'\n '     91' '     92' '     93' '     94' '     95' '     96' '     97'\n '     98' '     99' '     NA' '    100' '    101' '    102' '    103'\n '    104' '    105' '    106' '    107' '    108' '    109' '    110'\n '    111' '    112' '    113' '    114' '    115' '    116' '    117'\n '    118' '    119' '    120' '    121' '    122' '    123' '    124'\n '    125' '    126' '    127' '    128' '    129' '    130' '    131'\n '    132' '    133' '    134' '    135' '    136' '    137' '    138'\n '    139' '    140' '    141' '    142' '    143' '    144' '    145'\n '    146' '    147' '    148' '    149' '    150' '    151' '    152'\n '    153' '    154' '    155' '    156' '    157' '    158' '    159'\n '    160' '    161' '    162' '    163' '    164' '    165' '    166'\n '    167' '    168' '    169' '    170' '    171' '    172' '    173'\n '    174' '    175' '    176' '    177' '    178' '    179' '    180'\n '    181' '    182' '    183' '    184' '    185' '    186' '    187'\n '    188' '    189' '    190' '    191' '    192' '    193' '    194'\n '    195' '    196' '    197' '    198' '    199' '    200' '    201'\n '    202' '    203' '    204' '    205' '    206' '    207' '    208'\n '    209' '    210' '    211' '    212' '    213' '    214' '    215'\n '    216' '    217' '    218' '    219' '    220' '    221' '    222'\n '    223' '    224' '    225' '    226' '    227' '    228' '    229'\n '    230' '    231' '    232' '    233' '    234' '    235' '    236'\n '    237' '    238' '    239' '    240' '    241' '    242' '    243'\n '    244' '    245' '    246' '-999999' '0' '1' '10' '100' '101' '102'\n '103' '104' '105' '106' '107' '108' '109' '11' '110' '111' '112' '113'\n '114' '115' '116' '117' '118' '119' '12' '120' '121' '122' '123' '124'\n '125' '126' '127' '128' '129' '13' '130' '131' '132' '133' '134' '135'\n '136' '137' '138' '139' '14' '140' '141' '142' '143' '144' '145' '146'\n '147' '148' '149' '15' '150' '151' '152' '153' '154' '155' '156' '157'\n '158' '159' '16' '160' '161' '162' '163' '164' '165' '166' '167' '168'\n '169' '17' '170' '171' '172' '173' '174' '175' '176' '177' '178' '179'\n '18' '180' '181' '182' '183' '184' '185' '186' '187' '188' '189' '19'\n '190' '191' '192' '193' '194' '195' '196' '197' '198' '199' '2' '20'\n '200' '201' '202' '203' '204' '205' '206' '207' '208' '209' '21' '210'\n '211' '212' '213' '214' '215' '216' '217' '218' '219' '22' '220' '221'\n '222' '223' '224' '225' '226' '227' '228' '229' '23' '230' '231' '232'\n '233' '234' '235' '236' '237' '238' '239' '24' '240' '241' '242' '243'\n '244' '245' '246' '247' '248' '249' '25' '250' '251' '252' '253' '254'\n '255' '256' '26' '27' '28' '29' '3' '30' '31' '32' '33' '34' '35' '36'\n '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46' '47' '48' '49' '5'\n '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '6' '60' '61' '62' '63'\n '64' '65' '66' '67' '68' '69' '7' '70' '71' '72' '73' '74' '75' '76' '77'\n '78' '79' '8' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '9' '90'\n '91' '92' '93' '94' '95' '96' '97' '98' '99']\n--------------------------------------------------\n# col ult_fec_cli_1t, n_uniq 224, uniq ['2015-07-01' '2015-07-02' '2015-07-03' '2015-07-06' '2015-07-07'\n '2015-07-08' '2015-07-09' '2015-07-10' '2015-07-13' '2015-07-14'\n '2015-07-15' '2015-07-16' '2015-07-17' '2015-07-20' '2015-07-21'\n '2015-07-22' '2015-07-23' '2015-07-24' '2015-07-27' '2015-07-28'\n '2015-07-29' '2015-07-30' '2015-08-03' '2015-08-04' '2015-08-05'\n '2015-08-06' '2015-08-07' '2015-08-10' '2015-08-11' '2015-08-12'\n '2015-08-13' '2015-08-14' '2015-08-17' '2015-08-18' '2015-08-19'\n '2015-08-20' '2015-08-21' '2015-08-24' '2015-08-25' '2015-08-26'\n '2015-08-27' '2015-08-28' '2015-09-01' '2015-09-02' '2015-09-03'\n '2015-09-04' '2015-09-07' '2015-09-08' '2015-09-09' '2015-09-10'\n '2015-09-11' '2015-09-14' '2015-09-15' '2015-09-16' '2015-09-17'\n '2015-09-18' '2015-09-21' '2015-09-22' '2015-09-23' '2015-09-24'\n '2015-09-25' '2015-09-28' '2015-09-29' '2015-10-01' '2015-10-02'\n '2015-10-05' '2015-10-06' '2015-10-07' '2015-10-08' '2015-10-09'\n '2015-10-13' '2015-10-14' '2015-10-15' '2015-10-16' '2015-10-19'\n '2015-10-20' '2015-10-21' '2015-10-22' '2015-10-23' '2015-10-26'\n '2015-10-27' '2015-10-28' '2015-10-29' '2015-11-02' '2015-11-03'\n '2015-11-04' '2015-11-05' '2015-11-06' '2015-11-09' '2015-11-10'\n '2015-11-11' '2015-11-12' '2015-11-13' '2015-11-16' '2015-11-17'\n '2015-11-18' '2015-11-19' '2015-11-20' '2015-11-23' '2015-11-24'\n '2015-11-25' '2015-11-26' '2015-11-27' '2015-12-01' '2015-12-02'\n '2015-12-03' '2015-12-04' '2015-12-07' '2015-12-09' '2015-12-10'\n '2015-12-11' '2015-12-14' '2015-12-15' '2015-12-16' '2015-12-17'\n '2015-12-18' '2015-12-21' '2015-12-22' '2015-12-23' '2015-12-24'\n '2015-12-28' '2015-12-29' '2015-12-30' '2016-01-04' '2016-01-05'\n '2016-01-07' '2016-01-08' '2016-01-11' '2016-01-12' '2016-01-13'\n '2016-01-14' '2016-01-15' '2016-01-18' '2016-01-19' '2016-01-20'\n '2016-01-21' '2016-01-22' '2016-01-25' '2016-01-26' '2016-01-27'\n '2016-01-28' '2016-02-01' '2016-02-02' '2016-02-03' '2016-02-04'\n '2016-02-05' '2016-02-08' '2016-02-09' '2016-02-10' '2016-02-11'\n '2016-02-12' '2016-02-15' '2016-02-16' '2016-02-17' '2016-02-18'\n '2016-02-19' '2016-02-22' '2016-02-23' '2016-02-24' '2016-02-25'\n '2016-02-26' '2016-03-01' '2016-03-02' '2016-03-03' '2016-03-04'\n '2016-03-07' '2016-03-08' '2016-03-09' '2016-03-10' '2016-03-11'\n '2016-03-14' '2016-03-15' '2016-03-16' '2016-03-17' '2016-03-18'\n '2016-03-21' '2016-03-22' '2016-03-23' '2016-03-24' '2016-03-28'\n '2016-03-29' '2016-03-30' '2016-04-01' '2016-04-04' '2016-04-05'\n '2016-04-06' '2016-04-07' '2016-04-08' '2016-04-11' '2016-04-12'\n '2016-04-13' '2016-04-14' '2016-04-15' '2016-04-18' '2016-04-19'\n '2016-04-20' '2016-04-21' '2016-04-22' '2016-04-25' '2016-04-26'\n '2016-04-27' '2016-04-28' '2016-05-02' '2016-05-03' '2016-05-04'\n '2016-05-05' '2016-05-06' '2016-05-09' '2016-05-10' '2016-05-11'\n '2016-05-12' '2016-05-13' '2016-05-16' '2016-05-17' '2016-05-18'\n '2016-05-19' '2016-05-20' '2016-05-23' '2016-05-24' '2016-05-25'\n '2016-05-26' '2016-05-27' '2016-05-30' 'nan']\n--------------------------------------------------\n# col indrel_1mes, n_uniq 10, uniq ['1' '1.0' '2' '2.0' '3' '3.0' '4' '4.0' 'P' 'nan']\n--------------------------------------------------\n# col tiprel_1mes, n_uniq 6, uniq ['A' 'I' 'N' 'P' 'R' 'nan']\n--------------------------------------------------\n# col indresi, n_uniq 3, uniq ['N' 'S' 'nan']\n--------------------------------------------------\n# col indext, n_uniq 3, uniq ['N' 'S' 'nan']\n--------------------------------------------------\n# col conyuemp, n_uniq 3, uniq ['N' 'S' 'nan']\n--------------------------------------------------\n# col canal_entrada, n_uniq 163, uniq ['004' '007' '013' '025' 'K00' 'KAA' 'KAB' 'KAC' 'KAD' 'KAE' 'KAF' 'KAG'\n 'KAH' 'KAI' 'KAJ' 'KAK' 'KAL' 'KAM' 'KAN' 'KAO' 'KAP' 'KAQ' 'KAR' 'KAS'\n 'KAT' 'KAU' 'KAV' 'KAW' 'KAY' 'KAZ' 'KBB' 'KBD' 'KBE' 'KBF' 'KBG' 'KBH'\n 'KBJ' 'KBL' 'KBM' 'KBN' 'KBO' 'KBP' 'KBQ' 'KBR' 'KBS' 'KBU' 'KBV' 'KBW'\n 'KBX' 'KBY' 'KBZ' 'KCA' 'KCB' 'KCC' 'KCD' 'KCE' 'KCF' 'KCG' 'KCH' 'KCI'\n 'KCJ' 'KCK' 'KCL' 'KCM' 'KCN' 'KCO' 'KCP' 'KCQ' 'KCR' 'KCS' 'KCT' 'KCU'\n 'KCV' 'KCX' 'KDA' 'KDB' 'KDC' 'KDD' 'KDE' 'KDF' 'KDG' 'KDH' 'KDI' 'KDL'\n 'KDM' 'KDN' 'KDO' 'KDP' 'KDQ' 'KDR' 'KDS' 'KDT' 'KDU' 'KDV' 'KDW' 'KDX'\n 'KDY' 'KDZ' 'KEA' 'KEB' 'KEC' 'KED' 'KEE' 'KEF' 'KEG' 'KEH' 'KEI' 'KEJ'\n 'KEK' 'KEL' 'KEM' 'KEN' 'KEO' 'KEQ' 'KES' 'KEU' 'KEV' 'KEW' 'KEY' 'KEZ'\n 'KFA' 'KFB' 'KFC' 'KFD' 'KFE' 'KFF' 'KFG' 'KFH' 'KFI' 'KFJ' 'KFK' 'KFL'\n 'KFM' 'KFN' 'KFP' 'KFR' 'KFS' 'KFT' 'KFU' 'KFV' 'KGC' 'KGN' 'KGU' 'KGV'\n 'KGW' 'KGX' 'KGY' 'KHA' 'KHC' 'KHD' 'KHE' 'KHF' 'KHK' 'KHL' 'KHM' 'KHN'\n 'KHO' 'KHP' 'KHQ' 'KHR' 'KHS' 'RED' 'nan']\n--------------------------------------------------\n# col indfall, n_uniq 3, uniq ['N' 'S' 'nan']\n--------------------------------------------------\n# col nomprov, n_uniq 53, uniq ['ALAVA' 'ALBACETE' 'ALICANTE' 'ALMERIA' 'ASTURIAS' 'AVILA' 'BADAJOZ'\n 'BALEARS, ILLES' 'BARCELONA' 'BIZKAIA' 'BURGOS' 'CACERES' 'CADIZ'\n 'CANTABRIA' 'CASTELLON' 'CEUTA' 'CIUDAD REAL' 'CORDOBA' 'CORUÑA, A'\n 'CUENCA' 'GIPUZKOA' 'GIRONA' 'GRANADA' 'GUADALAJARA' 'HUELVA' 'HUESCA'\n 'JAEN' 'LEON' 'LERIDA' 'LUGO' 'MADRID' 'MALAGA' 'MELILLA' 'MURCIA'\n 'NAVARRA' 'OURENSE' 'PALENCIA' 'PALMAS, LAS' 'PONTEVEDRA' 'RIOJA, LA'\n 'SALAMANCA' 'SANTA CRUZ DE TENERIFE' 'SEGOVIA' 'SEVILLA' 'SORIA'\n 'TARRAGONA' 'TERUEL' 'TOLEDO' 'VALENCIA' 'VALLADOLID' 'ZAMORA' 'ZARAGOZA'\n 'nan']\n--------------------------------------------------\n# col segmento, n_uniq 4, uniq ['01 - TOP' '02 - PARTICULARES' '03 - UNIVERSITARIO' 'nan']"
  },
  {
    "objectID": "posts/Santander_ver1.html#데이터-분석",
    "href": "posts/Santander_ver1.html#데이터-분석",
    "title": "[Kaggle] Santander Product Recommendation Competition_v1",
    "section": "데이터 분석",
    "text": "데이터 분석\n\nfecha_dato: 년-월-일로 28일자를 기준으로 17개\nind_empleado: 결측치\npais_residencia: 나라(대문자 2개), 결측치\nsexo: 결측치\n\n,, 결측치 핵 많음\n\n\n\n변수명\n내용\n데이터 타입\n특징\n변수 아이디어\n\n\n\n\nfecha_data\n월별 날짜 데이터\nobject\n\n년도, 월 데이터 별도 추출\n\n\nage\n나이\nobeject -&gt;int\n\n나이데이터가 정수형이 아니므로 정제 필요\n\n\n\n\n위 내용은 책에서 정리한 거고.. 추후 이렇게 정리하는 분석 방법이 필요"
  },
  {
    "objectID": "posts/Santander_ver1.html#시각화",
    "href": "posts/Santander_ver1.html#시각화",
    "title": "[Kaggle] Santander Product Recommendation Competition_v1",
    "section": "시각화",
    "text": "시각화\n\nskip_cols = ['ncodpers', 'renta'] # 고객 고유 식별 번호, 총 수입 (고유값 많으므로 제거)\nfor col in df_train.columns:\n    if col in skip_cols:\n        continue\n    \n    print('-' * 50)\n    print('col:',col)\n    \n    f, ax = plt.subplots(figsize=(20,15))\n    sns.countplot(x=col, data=df_train, alpha=0.5)\n    plt.show()\n\n--------------------------------------------------\ncol: fecha_dato\n--------------------------------------------------\ncol: ind_empleado\n--------------------------------------------------\ncol: pais_residencia\n--------------------------------------------------\ncol: sexo\n--------------------------------------------------\ncol: age\n--------------------------------------------------\ncol: fecha_alta\n--------------------------------------------------\ncol: ind_nuevo\n--------------------------------------------------\ncol: antiguedad\n--------------------------------------------------\ncol: indrel\n--------------------------------------------------\ncol: ult_fec_cli_1t\n--------------------------------------------------\ncol: indrel_1mes\n--------------------------------------------------\ncol: tiprel_1mes\n--------------------------------------------------\ncol: indresi\n--------------------------------------------------\ncol: indext\n--------------------------------------------------\ncol: conyuemp\n--------------------------------------------------\ncol: canal_entrada\n--------------------------------------------------\ncol: indfall\n--------------------------------------------------\ncol: tipodom\n--------------------------------------------------\ncol: cod_prov\n--------------------------------------------------\ncol: nomprov\n--------------------------------------------------\ncol: ind_actividad_cliente\n--------------------------------------------------\ncol: segmento\n--------------------------------------------------\ncol: ind_ahor_fin_ult1\n--------------------------------------------------\ncol: ind_aval_fin_ult1\n--------------------------------------------------\ncol: ind_cco_fin_ult1\n--------------------------------------------------\ncol: ind_cder_fin_ult1\n--------------------------------------------------\ncol: ind_cno_fin_ult1\n--------------------------------------------------\ncol: ind_ctju_fin_ult1\n--------------------------------------------------\ncol: ind_ctma_fin_ult1\n--------------------------------------------------\ncol: ind_ctop_fin_ult1\n--------------------------------------------------\ncol: ind_ctpp_fin_ult1\n--------------------------------------------------\ncol: ind_deco_fin_ult1\n--------------------------------------------------\ncol: ind_deme_fin_ult1\n--------------------------------------------------\ncol: ind_dela_fin_ult1\n--------------------------------------------------\ncol: ind_ecue_fin_ult1\n--------------------------------------------------\ncol: ind_fond_fin_ult1\n--------------------------------------------------\ncol: ind_hip_fin_ult1\n--------------------------------------------------\ncol: ind_plan_fin_ult1\n--------------------------------------------------\ncol: ind_pres_fin_ult1\n--------------------------------------------------\ncol: ind_reca_fin_ult1\n--------------------------------------------------\ncol: ind_tjcr_fin_ult1\n--------------------------------------------------\ncol: ind_valo_fin_ult1\n--------------------------------------------------\ncol: ind_viv_fin_ult1\n--------------------------------------------------\ncol: ind_nomina_ult1\n--------------------------------------------------\ncol: ind_nom_pens_ult1\n--------------------------------------------------\ncol: ind_recibo_ult1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n월별 금융 제품 보유 데이터 누적 막대 그래프 시각화\n\nlabel_over_time = []\nfor i in range(len(label_cols)):\n    label_sum = df_train.groupby(['fecha_dato'])[label_cols[i]].agg('sum')\n    label_over_time.append(label_sum.tolist())\n    \nlabel_sum_over_time = []\nfor i in range(len(label_cols)):\n    label_sum_over_time.append(np.asarray(label_over_time[i:]).sum(axis=0))\n\ncolor_list = ['#F5B7B1','#D2B4DE','#AED6F1','#A2D9CE','#ABEBC6','#F9E79F','#F5CBA7','#CCD1D1']\n\n\nf, ax = plt.subplots(figsize=(30, 15))\nfor i in range(len(label_cols)):\n    sns.barplot(x=months, y=label_sum_over_time[i], color = color_list[i%8], alpha=0.7)\n\n\nplt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], label_cols, loc=1, ncol = 2, prop={'size':16})\n\n&lt;matplotlib.legend.Legend at 0x7f1f78271070&gt;\n\n\n\n\n\n\n\n누적 막대 그래프 상대값 시각화\n\nlabel_sum_percent = (label_sum_over_time / (1.*np.asarray(label_sum_over_time).max(axis=0))) * 100\n\n\nf, ax = plt.subplots(figsize=(30, 15))\nfor i in range(len(label_cols)):\n    sns.barplot(x=months, y=label_sum_percent[i], color = color_list[i%8], alpha=0.7)\n    \nplt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], \\\n           label_cols, loc=1, ncol = 2, prop={'size':16})\n\n&lt;matplotlib.legend.Legend at 0x7f1f770de610&gt;\n\n\n\n\n\n\n상대값으로 보기 위해서 퍼센테이지 나눈거!\n\n\n\n신규 구매 데이터 생성 및 시각화\n\n책에서는 예측해야할 ’신규 구매’에 초점을 맞추고 있어서 새로운 데이터를 추출했다.—&gt; 일단 난 안할래 흠냐\n\n\n# 제품 변수를 prods에 list형태로 저장한다\nprods = trn.columns[24:].tolist()\n\n# 날짜를 숫자로 변환하는 함수이다. 2015-01-28은 1, 2016-06-28은 18로 변환된다\ndef date_to_int(str_date):\n    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n    int_date = (int(Y) - 2015) * 12 + int(M)\n    return int_date\n\n# 날짜를 숫자로 변환하여 int_date에 저장한다\ntrn['int_date'] = trn['fecha_dato'].map(date_to_int).astype(np.int8)\n\n# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성한다. 변수명에 _prev를 추가한다.\ntrn_lag = trn.copy()\ntrn_lag['int_date'] += 1\ntrn_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in trn.columns]\n\n# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합친다. Lag 데이터의 int_date는 1 밀려있기 때문에, 저번달의 제품 정보가 삽입된다.\ndf_trn = trn.merge(trn_lag, on=['ncodpers','int_date'], how='left')\n\n# 메모리 효율을 위해 불필요한 변수를 메모리에서 제거한다\ndel trn, trn_lag\n\n# 저번달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체한다.\nfor prod in prods:\n    prev = prod + '_prev'\n    df_trn[prev].fillna(0, inplace=True)\n\n# 원본 데이터에서의 제품 보유 여부 – lag데이터에서의 제품 보유 여부를 비교하여 신규 구매 변수 padd를 구한다\nfor prod in prods:\n    padd = prod + '_add'\n    prev = prod + '_prev'\n    df_trn[padd] = ((df_trn[prod] == 1) & (df_trn[prev] == 0)).astype(np.int8)\n\n# 신규 구매 변수만을 추출하여 labels에 저장한다.\nadd_cols = [prod + '_add' for prod in prods]\nlabels = df_trn[add_cols].copy()\nlabels.columns = prods\n#labels.to_csv('../input/labels.csv', index=False)\n\n\n# 코드 1-12. 신규 구매 누적 막대 그래프를 시각화하기\n\n#labels = pd.read_csv('../input/labels.csv').astype(int)\nfecha_dato = trn['fecha_dato']\n\nlabels['date'] = fecha_dato.fecha_dato\nmonths = np.unique(fecha_dato.fecha_dato).tolist()\nlabel_cols = labels.columns.tolist()[:24]\n\nlabel_over_time = []\nfor i in range(len(label_cols)):\n    label_over_time.append(labels.groupby(['date'])[label_cols[i]].agg('sum').tolist())\n    \nlabel_sum_over_time = []\nfor i in range(len(label_cols)):\n    label_sum_over_time.append(np.asarray(label_over_time[i:]).sum(axis=0))\n    \ncolor_list = ['#F5B7B1','#D2B4DE','#AED6F1','#A2D9CE','#ABEBC6','#F9E79F','#F5CBA7','#CCD1D1']\n\nf, ax = plt.subplots(figsize=(30, 15))\nfor i in range(len(label_cols)):\n    sns.barplot(x=months, y=label_sum_over_time[i], color = color_list[i%8], alpha=0.7)\n    \nplt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], label_cols, loc=1, ncol = 2, prop={'size':16})\n\n&lt;matplotlib.legend.Legend at 0x7fb5f6211550&gt;\n\n\n\n\n\n\n# 코드 1-13. 신규 구매 누적 막대 그래프를 상대값으로 시각화하기\n\nlabel_sum_percent = (label_sum_over_time / (1.*np.asarray(label_sum_over_time).max(axis=0))) * 100\n\nf, ax = plt.subplots(figsize=(30, 15))\nfor i in range(len(label_cols)):\n    sns.barplot(x=months, y=label_sum_percent[i], color = color_list[i%8], alpha=0.7)\n    \nplt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], \\\n           label_cols, loc=1, ncol = 2, prop={'size':16})\n\n&lt;matplotlib.legend.Legend at 0x7fb5f682a898&gt;"
  },
  {
    "objectID": "posts/Santander_ver2.html",
    "href": "posts/Santander_ver2.html",
    "title": "[Kaggle] 02. Santander Product Recommendation Competition",
    "section": "",
    "text": "ref\n[Kaggle] Santander Product Recommendation Competition\nkaggle 우승작으로 배우는 머신러닝 탐구생활\n\n\nimport\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\n\n#---#\nfrom autogluon.multimodal import MultiModalPredictor # from autogluon.tabular import TabularPredictor\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\ndata\n\n!kaggle competitions download -c santander-product-recommendation\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\nsantander-product-recommendation.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\n\n!unzip santander-product-recommendation.zip -d data \n\nArchive:  santander-product-recommendation.zip\n  inflating: data/sample_submission.csv.zip  \n  inflating: data/test_ver2.csv.zip  \n  inflating: data/train_ver2.csv.zip  \n\n\n\ndf_train = pd.read_csv('data/train_ver2.csv.zip')\ndf_test = pd.read_csv('data/test_ver2.csv.zip')\nsample_submission = pd.read_csv('data/sample_submission.csv.zip')\n\n\n!rm -rf data\n!rm nlp-getting-started.zip\n\nrm: cannot remove 'nlp-getting-started.zip': No such file or directory\n\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nfecha_dato\nncodpers\nind_empleado\npais_residencia\nsexo\nage\nfecha_alta\nind_nuevo\nantiguedad\nindrel\n...\nind_hip_fin_ult1\nind_plan_fin_ult1\nind_pres_fin_ult1\nind_reca_fin_ult1\nind_tjcr_fin_ult1\nind_valo_fin_ult1\nind_viv_fin_ult1\nind_nomina_ult1\nind_nom_pens_ult1\nind_recibo_ult1\n\n\n\n\n0\n2015-01-28\n1375586\nN\nES\nH\n35\n2015-01-12\n0.0\n6\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n1\n2015-01-28\n1050611\nN\nES\nV\n23\n2012-08-10\n0.0\n35\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n2\n2015-01-28\n1050612\nN\nES\nV\n23\n2012-08-10\n0.0\n35\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n3\n2015-01-28\n1050613\nN\nES\nH\n22\n2012-08-10\n0.0\n35\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n4\n2015-01-28\n1050614\nN\nES\nV\n23\n2012-08-10\n0.0\n35\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n\n\n5 rows × 48 columns\n\n\n\n\n\n데이터전처리\n\nprods = df_train.columns[24:].tolist()\n\n\ndf_train[prods] = df_train[prods].fillna(0.0).astype(np.int8)\n\n- 24개 제품 중 하나도 보유하지 않는 고객 데이터 제거\n\nno_product = df_train[prods].sum(axis=1) == 0\ndf_train = df_train[~no_product]\n\n\ndf_train[~변수명]을 사용하면 제거!\n\n- tr/ts 합치기\n\nfor col in df_train.columns[24:]:\n    df_test[col] = 0\ndf = pd.concat([df_train,df_test], axis=0)\n\n\nfeatures = []\n\n\ncategorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\nfor col in categorical_cols:\n    df[col], _ = df[col].factorize(na_sentinel=-99)\nfeatures += categorical_cols\n\n\ndf['age'].replace(' NA', -99, inplace=True)\ndf['age'] = df['age'].astype(np.int8)\n\ndf['antiguedad'].replace('     NA', -99, inplace=True)\ndf['antiguedad'] = df['antiguedad'].astype(np.int8)\n\ndf['renta'].replace('         NA', -99, inplace=True)\ndf['renta'].fillna(-99, inplace=True)\ndf['renta'] = df['renta'].astype(float).astype(np.int8)\n\ndf['indrel_1mes'].replace('P', 5, inplace=True)\ndf['indrel_1mes'].fillna(-99, inplace=True)\ndf['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n\n\nfeatures += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']\n\n- 날짜 변수 월/연 추출\n\ndf['fecha_alta_month'] =  df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n\n\ndf['fecah_alta_year'] = df['fecha_alta'].map(lambda x:0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n\n\nfeatures += ['fecha_alta_month','fecah_alta_year']\n\n\ndf['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\ndf['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\nfeatures += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n\n- 그 외 변수 결측값: -99\n\ndf.fillna(-99, inplace=True)\n\n- 날짜 숫자로 변환\n\n2015-01-28:1, 2015-02-28:2, …, 2016-06-28:18\n\n\ndef date_to_int(str_date):\n    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n    int_date = (int(Y) - 2015) * 12 + int(M)\n    return int_date\n\n\ndf['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n\n\ndf_lag = df.copy()\ndf_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns]\ndf_lag['int_date'] += 1\n\n\ndf_trn = df.merge(df_lag, on=['ncodpers', 'int_date'], how='left')\n\n- 저번 달 제품 정보가 존재하지 않으면 0으로 대체\n\nfor prod in prods:\n    prev = prod + '_prev'\n    df_trn[prev].fillna(0, inplace=True)\ndf_trn.fillna(-99, inplace=True)\n\n- lag-1 변수 추가\n\nfeatures += [feature + '_prev' for feature in features]\nfeatures += [prod + '_prev' for prod in prods]\n\n\n\ntr/test/val\n\nuse_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n\n\ntrn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\ntst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\ndel df_trn\n\n- 훈련 데이터에 신규 구매 건수 추출\n\nX = []\nY = []\nfor i, prod in enumerate(prods):\n    prev = prod + '_prev'\n    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n    X.append(prX)\n    Y.append(prY)\nXY = pd.concat(X)\nY = np.hstack(Y)\nXY['y'] = Y\n\n\nvld_date = '2016-05-28'\nXY_trn = XY[XY['fecha_dato'] != vld_date]\nXY_vld = XY[XY['fecha_dato'] == vld_date]\n\n\n\n학습\n\nXGBoost\nmax_depth: 트리 모델의 최대 깊이\neta: 학습률\ncolsample_bytree: 변수 샘플링 기준, 보통 0.6~0.9\ncolsample_bylevel: 트리의 레벨 별 훈련 데이터의 변수 샘플링해주는 비율\n\n- parameter 설정\n\nparam = {\n    'booster': 'gbtree',\n    'max_depth': 8,\n    'nthread': 4,\n    'num_class': len(prods),\n    'objective': 'multi:softprob',\n    'silent': 1,\n    'eval_metric': 'mlogloss',\n    'eta': 0.1,\n    'min_child_weight': 10,\n    'colsample_bytree': 0.8,\n    'colsample_bylevel': 0.9,\n    'seed': 2018,\n    }\n\n\nXY_trn.ma\n\n\n\n\n\n\n\n\nfecha_dato\nncodpers\nind_empleado\npais_residencia\nsexo\nage\nfecha_alta\nind_nuevo\nantiguedad\nindrel\n...\nind_valo_fin_ult1_prev\nind_viv_fin_ult1_prev\nind_nomina_ult1_prev\nind_nom_pens_ult1_prev\nind_recibo_ult1_prev\nfecha_alta_month_prev\nfecah_alta_year_prev\nult_fec_cli_1t_month_prev\nult_fec_cli_1t_year_prev\ny\n\n\n\n\n7658069\n2016-01-28\n1474324\n0\n0\n1\n43\n2015-10-09\n1.0\n3\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n10.0\n2015.0\n0.0\n0.0\n1\n\n\n7628180\n2016-01-28\n1432311\n0\n0\n1\n26\n2015-08-07\n1.0\n5\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n-99.0\n-99.0\n-99.0\n-99.0\n2\n\n\n7628198\n2016-01-28\n1432232\n0\n0\n1\n33\n2015-08-07\n0.0\n19\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n-99.0\n-99.0\n-99.0\n-99.0\n2\n\n\n7628482\n2016-01-28\n1432080\n0\n0\n0\n23\n2015-08-07\n1.0\n5\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n-99.0\n-99.0\n-99.0\n-99.0\n2\n\n\n7628692\n2016-01-28\n1432952\n0\n0\n0\n77\n2015-08-10\n1.0\n5\n1.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n8.0\n2015.0\n0.0\n0.0\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10394466\n2016-04-28\n1297367\n0\n0\n1\n59\n2014-08-18\n0.0\n20\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n2014.0\n0.0\n0.0\n23\n\n\n10394481\n2016-04-28\n1297315\n0\n0\n1\n39\n2014-08-18\n0.0\n20\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n2014.0\n0.0\n0.0\n23\n\n\n10394487\n2016-04-28\n1297332\n0\n0\n1\n30\n2014-08-18\n0.0\n20\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n2014.0\n0.0\n0.0\n23\n\n\n10394502\n2016-04-28\n1297428\n0\n0\n0\n35\n2014-08-18\n0.0\n25\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n2014.0\n0.0\n0.0\n23\n\n\n10394529\n2016-04-28\n1297412\n0\n0\n1\n34\n2014-08-18\n0.0\n20\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n2014.0\n0.0\n0.0\n23\n\n\n\n\n161137 rows × 105 columns\n\n\n\n\nX_trn = XY_trn[features].values\nY_trn = XY_trn['y'].values\ndtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n\nX_vld = XY_vld[features].values\nY_vld = XY_vld['y'].values\ndvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n\n\nwatch_list = [(dtrn, 'train'), (dvld, 'eval')]\nmodel = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n\n[17:51:53] WARNING: ../src/learner.cc:767: \nParameters: { \"silent\" } are not used.\n\n[0] train-mlogloss:2.68947  eval-mlogloss:2.70092\n[1] train-mlogloss:2.45325  eval-mlogloss:2.46499\n[2] train-mlogloss:2.27879  eval-mlogloss:2.29092\n[3] train-mlogloss:2.14556  eval-mlogloss:2.16003\n[4] train-mlogloss:2.03233  eval-mlogloss:2.04872\n[5] train-mlogloss:1.93794  eval-mlogloss:1.95545\n[6] train-mlogloss:1.85559  eval-mlogloss:1.87363\n[7] train-mlogloss:1.78461  eval-mlogloss:1.80326\n[8] train-mlogloss:1.72369  eval-mlogloss:1.74273\n[9] train-mlogloss:1.67043  eval-mlogloss:1.69015\n[10]    train-mlogloss:1.62181  eval-mlogloss:1.64162\n[11]    train-mlogloss:1.57870  eval-mlogloss:1.59880\n[12]    train-mlogloss:1.54061  eval-mlogloss:1.56100\n[13]    train-mlogloss:1.50500  eval-mlogloss:1.52598\n[14]    train-mlogloss:1.47335  eval-mlogloss:1.49488\n[15]    train-mlogloss:1.44403  eval-mlogloss:1.46575\n[16]    train-mlogloss:1.41845  eval-mlogloss:1.44053\n[17]    train-mlogloss:1.39342  eval-mlogloss:1.41575\n[18]    train-mlogloss:1.37211  eval-mlogloss:1.39459\n[19]    train-mlogloss:1.35270  eval-mlogloss:1.37554\n[20]    train-mlogloss:1.33333  eval-mlogloss:1.35648\n[21]    train-mlogloss:1.31591  eval-mlogloss:1.33916\n[22]    train-mlogloss:1.30018  eval-mlogloss:1.32380\n[23]    train-mlogloss:1.28495  eval-mlogloss:1.30887\n[24]    train-mlogloss:1.27107  eval-mlogloss:1.29518\n[25]    train-mlogloss:1.25847  eval-mlogloss:1.28290\n[26]    train-mlogloss:1.24638  eval-mlogloss:1.27103\n[27]    train-mlogloss:1.23520  eval-mlogloss:1.26012\n[28]    train-mlogloss:1.22499  eval-mlogloss:1.25028\n[29]    train-mlogloss:1.21556  eval-mlogloss:1.24139\n[30]    train-mlogloss:1.20649  eval-mlogloss:1.23269\n[31]    train-mlogloss:1.19831  eval-mlogloss:1.22480\n[32]    train-mlogloss:1.19049  eval-mlogloss:1.21743\n[33]    train-mlogloss:1.18344  eval-mlogloss:1.21077\n[34]    train-mlogloss:1.17619  eval-mlogloss:1.20392\n[35]    train-mlogloss:1.16950  eval-mlogloss:1.19754\n[36]    train-mlogloss:1.16325  eval-mlogloss:1.19175\n[37]    train-mlogloss:1.15747  eval-mlogloss:1.18643\n[38]    train-mlogloss:1.15168  eval-mlogloss:1.18111\n[39]    train-mlogloss:1.14627  eval-mlogloss:1.17612\n[40]    train-mlogloss:1.14141  eval-mlogloss:1.17162\n[41]    train-mlogloss:1.13655  eval-mlogloss:1.16708\n[42]    train-mlogloss:1.13203  eval-mlogloss:1.16290\n[43]    train-mlogloss:1.12802  eval-mlogloss:1.15938\n[44]    train-mlogloss:1.12406  eval-mlogloss:1.15580\n[45]    train-mlogloss:1.12017  eval-mlogloss:1.15236\n[46]    train-mlogloss:1.11651  eval-mlogloss:1.14907\n[47]    train-mlogloss:1.11301  eval-mlogloss:1.14614\n[48]    train-mlogloss:1.10974  eval-mlogloss:1.14323\n[49]    train-mlogloss:1.10664  eval-mlogloss:1.14053\n[50]    train-mlogloss:1.10371  eval-mlogloss:1.13796\n[51]    train-mlogloss:1.10103  eval-mlogloss:1.13574\n[52]    train-mlogloss:1.09843  eval-mlogloss:1.13355\n[53]    train-mlogloss:1.09584  eval-mlogloss:1.13136\n[54]    train-mlogloss:1.09342  eval-mlogloss:1.12943\n[55]    train-mlogloss:1.09101  eval-mlogloss:1.12737\n[56]    train-mlogloss:1.08873  eval-mlogloss:1.12562\n[57]    train-mlogloss:1.08636  eval-mlogloss:1.12386\n[58]    train-mlogloss:1.08435  eval-mlogloss:1.12223\n[59]    train-mlogloss:1.08241  eval-mlogloss:1.12074\n[60]    train-mlogloss:1.08043  eval-mlogloss:1.11916\n[61]    train-mlogloss:1.07846  eval-mlogloss:1.11763\n[62]    train-mlogloss:1.07669  eval-mlogloss:1.11620\n[63]    train-mlogloss:1.07490  eval-mlogloss:1.11491\n[64]    train-mlogloss:1.07326  eval-mlogloss:1.11365\n[65]    train-mlogloss:1.07174  eval-mlogloss:1.11257\n[66]    train-mlogloss:1.07017  eval-mlogloss:1.11150\n[67]    train-mlogloss:1.06875  eval-mlogloss:1.11046\n[68]    train-mlogloss:1.06726  eval-mlogloss:1.10943\n[69]    train-mlogloss:1.06587  eval-mlogloss:1.10842\n[70]    train-mlogloss:1.06443  eval-mlogloss:1.10761\n[71]    train-mlogloss:1.06320  eval-mlogloss:1.10675\n[72]    train-mlogloss:1.06210  eval-mlogloss:1.10597\n[73]    train-mlogloss:1.06084  eval-mlogloss:1.10515\n[74]    train-mlogloss:1.05970  eval-mlogloss:1.10451\n[75]    train-mlogloss:1.05853  eval-mlogloss:1.10374\n[76]    train-mlogloss:1.05740  eval-mlogloss:1.10305\n[77]    train-mlogloss:1.05628  eval-mlogloss:1.10243\n[78]    train-mlogloss:1.05520  eval-mlogloss:1.10186\n[79]    train-mlogloss:1.05409  eval-mlogloss:1.10132\n[80]    train-mlogloss:1.05311  eval-mlogloss:1.10074\n[81]    train-mlogloss:1.05214  eval-mlogloss:1.10026\n[82]    train-mlogloss:1.05108  eval-mlogloss:1.09970\n[83]    train-mlogloss:1.05016  eval-mlogloss:1.09912\n[84]    train-mlogloss:1.04908  eval-mlogloss:1.09859\n[85]    train-mlogloss:1.04827  eval-mlogloss:1.09812\n[86]    train-mlogloss:1.04739  eval-mlogloss:1.09769\n[87]    train-mlogloss:1.04654  eval-mlogloss:1.09727\n[88]    train-mlogloss:1.04570  eval-mlogloss:1.09689\n[89]    train-mlogloss:1.04478  eval-mlogloss:1.09649\n[90]    train-mlogloss:1.04391  eval-mlogloss:1.09613\n[91]    train-mlogloss:1.04309  eval-mlogloss:1.09579\n[92]    train-mlogloss:1.04230  eval-mlogloss:1.09548\n[93]    train-mlogloss:1.04152  eval-mlogloss:1.09519\n[94]    train-mlogloss:1.04072  eval-mlogloss:1.09484\n[95]    train-mlogloss:1.04003  eval-mlogloss:1.09456\n[96]    train-mlogloss:1.03926  eval-mlogloss:1.09429\n[97]    train-mlogloss:1.03849  eval-mlogloss:1.09404\n[98]    train-mlogloss:1.03778  eval-mlogloss:1.09376\n[99]    train-mlogloss:1.03714  eval-mlogloss:1.09352\n[100]   train-mlogloss:1.03645  eval-mlogloss:1.09329\n[101]   train-mlogloss:1.03578  eval-mlogloss:1.09307\n[102]   train-mlogloss:1.03522  eval-mlogloss:1.09283\n[103]   train-mlogloss:1.03463  eval-mlogloss:1.09256\n[104]   train-mlogloss:1.03393  eval-mlogloss:1.09233\n[105]   train-mlogloss:1.03315  eval-mlogloss:1.09210\n[106]   train-mlogloss:1.03244  eval-mlogloss:1.09192\n[107]   train-mlogloss:1.03189  eval-mlogloss:1.09173\n[108]   train-mlogloss:1.03125  eval-mlogloss:1.09155\n[109]   train-mlogloss:1.03053  eval-mlogloss:1.09139\n[110]   train-mlogloss:1.02985  eval-mlogloss:1.09124\n[111]   train-mlogloss:1.02925  eval-mlogloss:1.09107\n[112]   train-mlogloss:1.02862  eval-mlogloss:1.09089\n[113]   train-mlogloss:1.02805  eval-mlogloss:1.09078\n[114]   train-mlogloss:1.02734  eval-mlogloss:1.09064\n[115]   train-mlogloss:1.02655  eval-mlogloss:1.09049\n[116]   train-mlogloss:1.02593  eval-mlogloss:1.09034\n[117]   train-mlogloss:1.02528  eval-mlogloss:1.09021\n[118]   train-mlogloss:1.02473  eval-mlogloss:1.09011\n[119]   train-mlogloss:1.02409  eval-mlogloss:1.09003\n[120]   train-mlogloss:1.02350  eval-mlogloss:1.08995\n[121]   train-mlogloss:1.02297  eval-mlogloss:1.08986\n[122]   train-mlogloss:1.02245  eval-mlogloss:1.08976\n[123]   train-mlogloss:1.02182  eval-mlogloss:1.08969\n[124]   train-mlogloss:1.02116  eval-mlogloss:1.08958\n[125]   train-mlogloss:1.02049  eval-mlogloss:1.08951\n[126]   train-mlogloss:1.01983  eval-mlogloss:1.08939\n[127]   train-mlogloss:1.01928  eval-mlogloss:1.08928\n[128]   train-mlogloss:1.01866  eval-mlogloss:1.08921\n[129]   train-mlogloss:1.01810  eval-mlogloss:1.08906\n[130]   train-mlogloss:1.01736  eval-mlogloss:1.08893\n[131]   train-mlogloss:1.01683  eval-mlogloss:1.08883\n[132]   train-mlogloss:1.01634  eval-mlogloss:1.08878\n[133]   train-mlogloss:1.01579  eval-mlogloss:1.08873\n[134]   train-mlogloss:1.01526  eval-mlogloss:1.08865\n[135]   train-mlogloss:1.01460  eval-mlogloss:1.08852\n[136]   train-mlogloss:1.01403  eval-mlogloss:1.08848\n[137]   train-mlogloss:1.01352  eval-mlogloss:1.08843\n[138]   train-mlogloss:1.01279  eval-mlogloss:1.08834\n[139]   train-mlogloss:1.01231  eval-mlogloss:1.08829\n[140]   train-mlogloss:1.01172  eval-mlogloss:1.08824\n[141]   train-mlogloss:1.01118  eval-mlogloss:1.08820\n[142]   train-mlogloss:1.01062  eval-mlogloss:1.08812\n[143]   train-mlogloss:1.01010  eval-mlogloss:1.08803\n[144]   train-mlogloss:1.00950  eval-mlogloss:1.08799\n[145]   train-mlogloss:1.00900  eval-mlogloss:1.08794\n[146]   train-mlogloss:1.00853  eval-mlogloss:1.08789\n[147]   train-mlogloss:1.00807  eval-mlogloss:1.08784\n[148]   train-mlogloss:1.00755  eval-mlogloss:1.08781\n[149]   train-mlogloss:1.00691  eval-mlogloss:1.08776\n[150]   train-mlogloss:1.00629  eval-mlogloss:1.08774\n[151]   train-mlogloss:1.00562  eval-mlogloss:1.08766\n[152]   train-mlogloss:1.00517  eval-mlogloss:1.08763\n[153]   train-mlogloss:1.00449  eval-mlogloss:1.08754\n[154]   train-mlogloss:1.00404  eval-mlogloss:1.08752\n[155]   train-mlogloss:1.00352  eval-mlogloss:1.08747\n[156]   train-mlogloss:1.00294  eval-mlogloss:1.08742\n[157]   train-mlogloss:1.00235  eval-mlogloss:1.08735\n[158]   train-mlogloss:1.00190  eval-mlogloss:1.08737\n[159]   train-mlogloss:1.00144  eval-mlogloss:1.08731\n[160]   train-mlogloss:1.00084  eval-mlogloss:1.08728\n[161]   train-mlogloss:1.00024  eval-mlogloss:1.08726\n[162]   train-mlogloss:0.99963  eval-mlogloss:1.08723\n[163]   train-mlogloss:0.99917  eval-mlogloss:1.08726\n[164]   train-mlogloss:0.99848  eval-mlogloss:1.08727\n[165]   train-mlogloss:0.99792  eval-mlogloss:1.08723\n[166]   train-mlogloss:0.99743  eval-mlogloss:1.08721\n[167]   train-mlogloss:0.99683  eval-mlogloss:1.08718\n[168]   train-mlogloss:0.99622  eval-mlogloss:1.08719\n[169]   train-mlogloss:0.99559  eval-mlogloss:1.08715\n[170]   train-mlogloss:0.99502  eval-mlogloss:1.08717\n[171]   train-mlogloss:0.99459  eval-mlogloss:1.08714\n[172]   train-mlogloss:0.99398  eval-mlogloss:1.08707\n[173]   train-mlogloss:0.99335  eval-mlogloss:1.08699\n[174]   train-mlogloss:0.99277  eval-mlogloss:1.08697\n[175]   train-mlogloss:0.99213  eval-mlogloss:1.08695\n[176]   train-mlogloss:0.99156  eval-mlogloss:1.08695\n[177]   train-mlogloss:0.99105  eval-mlogloss:1.08690\n[178]   train-mlogloss:0.99045  eval-mlogloss:1.08688\n[179]   train-mlogloss:0.98992  eval-mlogloss:1.08686\n[180]   train-mlogloss:0.98923  eval-mlogloss:1.08683\n[181]   train-mlogloss:0.98876  eval-mlogloss:1.08680\n[182]   train-mlogloss:0.98814  eval-mlogloss:1.08679\n[183]   train-mlogloss:0.98757  eval-mlogloss:1.08675\n[184]   train-mlogloss:0.98707  eval-mlogloss:1.08678\n[185]   train-mlogloss:0.98656  eval-mlogloss:1.08680\n[186]   train-mlogloss:0.98600  eval-mlogloss:1.08678\n[187]   train-mlogloss:0.98555  eval-mlogloss:1.08679\n[188]   train-mlogloss:0.98491  eval-mlogloss:1.08678\n[189]   train-mlogloss:0.98429  eval-mlogloss:1.08675\n[190]   train-mlogloss:0.98373  eval-mlogloss:1.08672\n[191]   train-mlogloss:0.98317  eval-mlogloss:1.08676\n[192]   train-mlogloss:0.98268  eval-mlogloss:1.08670\n[193]   train-mlogloss:0.98213  eval-mlogloss:1.08667\n[194]   train-mlogloss:0.98147  eval-mlogloss:1.08663\n[195]   train-mlogloss:0.98090  eval-mlogloss:1.08659\n[196]   train-mlogloss:0.98040  eval-mlogloss:1.08655\n[197]   train-mlogloss:0.97987  eval-mlogloss:1.08656\n[198]   train-mlogloss:0.97937  eval-mlogloss:1.08653\n[199]   train-mlogloss:0.97892  eval-mlogloss:1.08649\n[200]   train-mlogloss:0.97848  eval-mlogloss:1.08648\n[201]   train-mlogloss:0.97792  eval-mlogloss:1.08652\n[202]   train-mlogloss:0.97742  eval-mlogloss:1.08652\n[203]   train-mlogloss:0.97700  eval-mlogloss:1.08652\n[204]   train-mlogloss:0.97654  eval-mlogloss:1.08652\n[205]   train-mlogloss:0.97612  eval-mlogloss:1.08653\n[206]   train-mlogloss:0.97572  eval-mlogloss:1.08654\n[207]   train-mlogloss:0.97514  eval-mlogloss:1.08648\n[208]   train-mlogloss:0.97472  eval-mlogloss:1.08645\n[209]   train-mlogloss:0.97415  eval-mlogloss:1.08638\n[210]   train-mlogloss:0.97366  eval-mlogloss:1.08635\n[211]   train-mlogloss:0.97321  eval-mlogloss:1.08637\n[212]   train-mlogloss:0.97274  eval-mlogloss:1.08634\n[213]   train-mlogloss:0.97227  eval-mlogloss:1.08636\n[214]   train-mlogloss:0.97182  eval-mlogloss:1.08637\n[215]   train-mlogloss:0.97142  eval-mlogloss:1.08640\n[216]   train-mlogloss:0.97105  eval-mlogloss:1.08637\n[217]   train-mlogloss:0.97060  eval-mlogloss:1.08630\n[218]   train-mlogloss:0.97028  eval-mlogloss:1.08634\n[219]   train-mlogloss:0.96980  eval-mlogloss:1.08627\n[220]   train-mlogloss:0.96934  eval-mlogloss:1.08625\n[221]   train-mlogloss:0.96883  eval-mlogloss:1.08628\n[222]   train-mlogloss:0.96837  eval-mlogloss:1.08630\n[223]   train-mlogloss:0.96800  eval-mlogloss:1.08635\n[224]   train-mlogloss:0.96752  eval-mlogloss:1.08638\n[225]   train-mlogloss:0.96716  eval-mlogloss:1.08639\n[226]   train-mlogloss:0.96686  eval-mlogloss:1.08639\n[227]   train-mlogloss:0.96648  eval-mlogloss:1.08641\n[228]   train-mlogloss:0.96591  eval-mlogloss:1.08638\n[229]   train-mlogloss:0.96536  eval-mlogloss:1.08637\n[230]   train-mlogloss:0.96486  eval-mlogloss:1.08638\n[231]   train-mlogloss:0.96440  eval-mlogloss:1.08636\n[232]   train-mlogloss:0.96395  eval-mlogloss:1.08633\n[233]   train-mlogloss:0.96350  eval-mlogloss:1.08635\n[234]   train-mlogloss:0.96280  eval-mlogloss:1.08639\n[235]   train-mlogloss:0.96212  eval-mlogloss:1.08630\n[236]   train-mlogloss:0.96175  eval-mlogloss:1.08636\n[237]   train-mlogloss:0.96135  eval-mlogloss:1.08637\n[238]   train-mlogloss:0.96081  eval-mlogloss:1.08640\n[239]   train-mlogloss:0.96025  eval-mlogloss:1.08643\n[240]   train-mlogloss:0.95976  eval-mlogloss:1.08641"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boram-coco",
    "section": "",
    "text": "Everyday with Coco"
  }
]